{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Good Are You At Guess Who?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Source: https://fivethirtyeight.com/features/how-good-are-you-at-guess-who/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "- The board has `N` characters.\n",
    "- Players choose a character as their own.\n",
    "- Players alternate turns.\n",
    "- At each turn, a player may:\n",
    "  - Make a specific guess about the opponent's character.\n",
    "    - A correct guess counts as a win.\n",
    "    - An incorrect guess counts as a loss.\n",
    "    - This is encoded as a `0`.\n",
    "  - Ask a yes/no question to reduce the possible characters.\n",
    "    - The players are good enough to create any division of remaining characters.\n",
    "    - This is encoded by the number of characters in the smaller remaining group.\n",
    "    - An example: an attempt to divide 4 players into 1 and 3 would be encoded as `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "What's the probability of the first player winning if:\n",
    "1. `N == 4`?\n",
    "1. `N == 24` (original game)?\n",
    "1. `N == 14`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "While there are other ways to solve this problem, this problem is a good fit for Reinforcement Learning. The following code uses the [Reinforce package](https://github.com/JuliaML/Reinforce.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mActivating\u001b[22m\u001b[39m environment at `~/vcs/notebooks/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\"..\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines the environment, policy, and state for reinforcement learning.\n",
    "\"\"\"\n",
    "module GuessWho\n",
    "\n",
    "import Reinforce: \n",
    "    AbstractEnvironment, AbstractPolicy, action, actions, finished, maxsteps, state, step!, reset!, reward\n",
    "\n",
    "export GuessWhoEnvironment, GuessWhoPolicy, GuessWhoState\n",
    "\n",
    "\"\"\"\n",
    "GuessWho game state as defined by the number of characters remaining on\n",
    "the player's board as well as the number of players on the opponent's\n",
    "board.\n",
    "\"\"\"\n",
    "struct GuessWhoState\n",
    "    ownchars::Int\n",
    "    oppchars::Int\n",
    "end\n",
    "\n",
    "GuessWhoState(n) = GuessWhoState(n, n)\n",
    "\n",
    "flipstate(s::GuessWhoState) = GuessWhoState(s.oppchars, s.ownchars)\n",
    "\n",
    "\"\"\"\n",
    "Parent class for policies that play GuessWho.\n",
    "\"\"\"\n",
    "abstract type GuessWhoPolicy <: AbstractPolicy end\n",
    "\n",
    "\"\"\"\n",
    "Reinforcement Learning environment that stores all the information needed \n",
    "to play GuessWho.\n",
    "\"\"\"\n",
    "mutable struct GuessWhoEnvironment <: AbstractEnvironment\n",
    "    n::Int  # number of characters\n",
    "    s::GuessWhoState  # state\n",
    "    r::Int  # reward\n",
    "    o::GuessWhoPolicy  # opponent policy\n",
    "    GuessWhoEnvironment(n, o) = new(n, GuessWhoState(n), 0, o)\n",
    "end\n",
    "\n",
    "reset!(env::GuessWhoEnvironment) = (env.s = GuessWhoState(env.n); env.r = 0; env)\n",
    "\n",
    "function makestep(s::GuessWhoState, a::Int)\n",
    "    if a == 0\n",
    "        # Guess and end game\n",
    "        r = rand() < (1 / s.ownchars) ? 1 : -1\n",
    "        s = GuessWhoState(0, 0)\n",
    "    else\n",
    "        r = 0\n",
    "        remaining = rand() < (a / s.ownchars) ? a : (s.ownchars - a)\n",
    "        s = GuessWhoState(remaining, s.oppchars)\n",
    "    end\n",
    "    return r, s\n",
    "end\n",
    "\n",
    "function step!(env::GuessWhoEnvironment, s::GuessWhoState, a::Int)\n",
    "    r, s = makestep(s, a)\n",
    "    if !finished(env, s)\n",
    "        # take random step as opponent\n",
    "        os = flipstate(s)\n",
    "        A = actions(env, os)\n",
    "        oa = action(env.o, 0, os, A)\n",
    "        or, os = makestep(os, oa)\n",
    "        r = -1 * or\n",
    "        s = flipstate(os)\n",
    "    end\n",
    "    env.r = r\n",
    "    env.s = s\n",
    "    return (env.r, env.s)\n",
    "end\n",
    "state(env::GuessWhoEnvironment) = env.s\n",
    "reward(env::GuessWhoEnvironment) = env.r\n",
    "maxsteps(env::GuessWhoEnvironment) = env.n + 1\n",
    "actions(env::GuessWhoEnvironment, s::GuessWhoState) = collect(0:(s.ownchars ÷ 2))\n",
    "finished(env::GuessWhoEnvironment, s′::GuessWhoState) = (s′.ownchars == 0 || s′.oppchars == 0)\n",
    "\n",
    "end  # module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A minimal implementation of a GuessWho policy (random play).\n",
    "\"\"\"\n",
    "module RandomGuessWho\n",
    "\n",
    "import Reinforce: action\n",
    "import Main.GuessWho: GuessWhoPolicy, GuessWhoState\n",
    "\n",
    "export RandomGuessWhoPolicy\n",
    "\n",
    "struct RandomGuessWhoPolicy <: GuessWhoPolicy end\n",
    "action(p::RandomGuessWhoPolicy, r::Int, s::GuessWhoState, A) = rand(A)\n",
    "\n",
    "end # module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A policy for GuessWho that improves from prior game play\n",
    "\"\"\"\n",
    "module LearningGuessWho\n",
    "\n",
    "import OnlineStats: fit!, Mean, value\n",
    "import Reinforce: action\n",
    "import Main.GuessWho: GuessWhoPolicy, GuessWhoState\n",
    "\n",
    "export LearningGuessWhoPolicy, learn!\n",
    "\n",
    "# define learning policy\n",
    "\"\"\"\n",
    "An ε-greedy GuessWhoPolicy\n",
    "\"\"\"\n",
    "struct LearningGuessWhoPolicy <: GuessWhoPolicy\n",
    "    ε::Float64\n",
    "    states::Dict\n",
    "end\n",
    "\n",
    "LearningGuessWhoPolicy(ε) = LearningGuessWhoPolicy(ε, Dict())\n",
    "LearningGuessWhoPolicy(p::LearningGuessWhoPolicy, ε) = LearningGuessWhoPolicy(ε, p.states)\n",
    "\n",
    "function action(p::LearningGuessWhoPolicy, r::Int, s::GuessWhoState, A)\n",
    "    if rand() < p.ε || !haskey(p.states, s)\n",
    "        return rand(A)\n",
    "    else\n",
    "        possible_rewards = [value(get(p.states[s], a, Mean())) for a in A]\n",
    "        exp_r, best_a = findmax(possible_rewards)\n",
    "        return A[best_a]\n",
    "    end\n",
    "end\n",
    "\n",
    "function learn!(p::LearningGuessWhoPolicy, r, states, actions)\n",
    "    for (s, a) in Iterators.zip(states, actions)\n",
    "        if !haskey(p.states, s)\n",
    "            p.states[s] = Dict()\n",
    "        end\n",
    "        if !haskey(p.states[s], a)\n",
    "            p.states[s][a] = Mean()\n",
    "        end\n",
    "        fit!(p.states[s][a], max(r, 0.0))\n",
    "    end\n",
    "end\n",
    "\n",
    "end  # module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an Episode\n",
    "\n",
    "At this point, game definition and play has been implemented. It's time to test the code by running a single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Reinforce: action, Episode, run_episode\n",
    "using  Main.GuessWho\n",
    "using  Main.RandomGuessWho\n",
    "using  Main.LearningGuessWho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Episode{GuessWhoEnvironment,LearningGuessWhoPolicy,Float64}(GuessWhoEnvironment(4, GuessWhoState(4, 4), 0, RandomGuessWhoPolicy()), LearningGuessWhoPolicy(0.1, Dict{Any,Any}()), 0.0, 0.0, 1, 1, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GuessWhoEnvironment(4, RandomGuessWhoPolicy())  # play against a random opponent\n",
    "p = LearningGuessWhoPolicy(0.1)  # create a learning policy (with ε = 0.1)\n",
    "ep = Episode(env, p)  # construct the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: GuessWhoState(4, 4)\n",
      "Action taken : 2\n",
      "Result       : 0\n",
      "Final state  : GuessWhoState(2, 3)\n",
      "\n",
      "Initial state: GuessWhoState(2, 3)\n",
      "Action taken : 1\n",
      "Result       : 0\n",
      "Final state  : GuessWhoState(1, 2)\n",
      "\n",
      "Initial state: GuessWhoState(1, 2)\n",
      "Action taken : 0\n",
      "Result       : 1\n",
      "Final state  : GuessWhoState(0, 0)\n",
      "\n",
      "Performed 3 iterations with a result of 1.0\n"
     ]
    }
   ],
   "source": [
    "# Store states and actions for learning purposes\n",
    "state_array = GuessWhoState[]\n",
    "action_array = Int[]\n",
    "\n",
    "# Iterate through the episode\n",
    "# See: https://github.com/JuliaML/Reinforce.jl#episode-iterator\n",
    "for (s, a, r, s′) in ep\n",
    "    push!(state_array, s)\n",
    "    println(\"Initial state: $s\")\n",
    "    push!(action_array, a)\n",
    "    println(\"Action taken : $a\")\n",
    "    println(\"Result       : $r\")\n",
    "    println(\"Final state  : $s′\")\n",
    "    println()\n",
    "end\n",
    "println(\"Performed $(ep.niter) iterations with a result of $(ep.total_reward)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A result (`ep.total_reward`) of -1.0 indicates that the player with policy `p` lost the game. This value&mdash;in combination with `state_array` and `action_array`&mdash;can be applied to `p` to improve future actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 1 entry:\n",
       "  2 => Mean: n=1 | value=1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn from the episode\n",
    "learn!(p, ep.total_reward, state_array, action_array)\n",
    "p.states[GuessWhoState(4, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Policy with Many Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train!"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trains `p` by running `num_episodes`, learning after each episode\n",
    "\"\"\"\n",
    "function train!(p::LearningGuessWhoPolicy, env::GuessWhoEnvironment, num_episodes::Int)\n",
    "    for i in 1:num_episodes\n",
    "        state_array = GuessWhoState[]\n",
    "        action_array = Int[]\n",
    "        R = run_episode(env, p) do (s, a, r, s′)\n",
    "            # called after each step\n",
    "            push!(state_array, s)\n",
    "            push!(action_array, a)\n",
    "        end\n",
    "        learn!(p, R, state_array, action_array)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conducts training of `num_policies` ε-greedy policies with each policy playing\n",
    "against the previous policy (except the first, which plays against a random \n",
    "policy)\n",
    "\"\"\"\n",
    "function training(numchars::Int, num_policies::Int, policy_episodes::Int, ε::Float64)\n",
    "    opponent = RandomGuessWhoPolicy()\n",
    "    policies = [LearningGuessWhoPolicy(ε) for i in 1:num_policies]\n",
    "    for i in 1:num_policies\n",
    "        println(\"Training policy $i\")\n",
    "        if i == 1\n",
    "            opponent = RandomGuessWhoPolicy()\n",
    "        else\n",
    "            opponent = LearningGuessWhoPolicy(0.0, policies[i-1].states)\n",
    "        end\n",
    "        env = GuessWhoEnvironment(numchars, opponent)\n",
    "        train!(policies[i], env, policy_episodes)\n",
    "    end\n",
    "    return policies\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training policy 1\n",
      "Training policy 2\n",
      "Training policy 3\n",
      "Training policy 4\n",
      "Training policy 5\n",
      "Training policy 6\n",
      "Training policy 7\n",
      "Training policy 8\n",
      "Training policy 9\n",
      "Training policy 10\n"
     ]
    }
   ],
   "source": [
    "# Train 10 policies, each with a million iterations to learn\n",
    "policies = training(4, 10, 1_000_000, 0.15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 3 entries:\n",
       "  0 => Mean: n=49660 | value=0.25439\n",
       "  2 => Mean: n=56306 | value=0.618993\n",
       "  1 => Mean: n=894034 | value=0.657091"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an optimal policy to infer results of optimal play\n",
    "policies[end].states[GuessWhoState(4, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first player wins approximately 66% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training policy 1\n",
      "Training policy 2\n",
      "Training policy 3\n",
      "Training policy 4\n",
      "Training policy 5\n",
      "Training policy 6\n",
      "Training policy 7\n",
      "Training policy 8\n",
      "Training policy 9\n",
      "Training policy 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 13 entries:\n",
       "  2  => Mean: n=11687 | value=0.530675\n",
       "  11 => Mean: n=11716 | value=0.589109\n",
       "  0  => Mean: n=11642 | value=0.0365058\n",
       "  7  => Mean: n=11616 | value=0.58032\n",
       "  9  => Mean: n=11648 | value=0.586453\n",
       "  10 => Mean: n=860800 | value=0.617142\n",
       "  8  => Mean: n=11494 | value=0.591787\n",
       "  6  => Mean: n=11542 | value=0.57737\n",
       "  4  => Mean: n=11698 | value=0.556334\n",
       "  3  => Mean: n=11444 | value=0.539759\n",
       "  5  => Mean: n=11465 | value=0.564501\n",
       "  12 => Mean: n=11486 | value=0.612833\n",
       "  1  => Mean: n=11762 | value=0.508757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let n = 24\n",
    "    policies = training(n, 10, 1_000_000, 0.15);\n",
    "    policies[end].states[GuessWhoState(n, n)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training policy 1\n",
      "Training policy 2\n",
      "Training policy 3\n",
      "Training policy 4\n",
      "Training policy 5\n",
      "Training policy 6\n",
      "Training policy 7\n",
      "Training policy 8\n",
      "Training policy 9\n",
      "Training policy 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 8 entries:\n",
       "  0 => Mean: n=19079 | value=0.0719639\n",
       "  7 => Mean: n=788658 | value=0.603865\n",
       "  4 => Mean: n=18710 | value=0.580866\n",
       "  2 => Mean: n=18569 | value=0.546664\n",
       "  3 => Mean: n=98684 | value=0.561773\n",
       "  5 => Mean: n=18812 | value=0.592707\n",
       "  6 => Mean: n=18612 | value=0.59381\n",
       "  1 => Mean: n=18876 | value=0.513774"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let n = 14\n",
    "    policies = training(n, 10, 1_000_000, 0.15);\n",
    "    policies[end].states[GuessWhoState(n, n)]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 24 characters, the first player wins about 62% of the time; with 14 players, 60% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact answers: 0.5625, 0.5555555555555556, 0.5612244897959183\n"
     ]
    }
   ],
   "source": [
    "println(\"Exact answers: $(9/16), $(5/9), $(55/98)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL results could probably get closer to the optimal results with parameter tuning and additional iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
